{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626897ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "def get_params_to_update(net, feature_extract):\n",
    "    params_to_update = net.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in net.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in net.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "    return params_to_update\n",
    "\n",
    "def make_alexnet(out_features, feature_extract = False):\n",
    "    net = models.alexnet(pretrained=True)\n",
    "    set_parameter_requires_grad(net, feature_extract)\n",
    "    num_ftrs = net.classifier[6].in_features\n",
    "    net.classifier[6] = nn.Linear(num_ftrs, out_features)\n",
    "    params_to_update = get_params_to_update(net, feature_extract)\n",
    "    optimizer = optim.Adam(params_to_update, lr=0.0001)\n",
    "    return net, optimizer\n",
    "\n",
    "def make_vgg16(out_features, feature_extract = False):\n",
    "    net = models.vgg16_bn(pretrained=True)\n",
    "    print(net)\n",
    "    set_parameter_requires_grad(net, feature_extract)\n",
    "    num_ftrs = net.classifier[6].in_features\n",
    "    net.classifier[6] = nn.Linear(num_ftrs, out_features)\n",
    "    params_to_update = get_params_to_update(net, feature_extract)\n",
    "    optimizer = optim.Adam(params_to_update, lr=0.00002)\n",
    "    return net, optimizer\n",
    "\n",
    "def make_mobilenet_v3(out_features, feature_extract = False):\n",
    "    net = models.mobilenet_v3_small(pretrained=True)\n",
    "    set_parameter_requires_grad(net, feature_extract)\n",
    "    num_ftrs = net.classifier[3].in_features\n",
    "    net.classifier[3] = nn.Linear(num_ftrs, out_features)\n",
    "    params_to_update = get_params_to_update(net, feature_extract)\n",
    "    optimizer = optim.Adam(params_to_update, lr=0.0001)\n",
    "    return net, optimizer\n",
    "\n",
    "def make_inception_v3(out_features, feature_extract = False):\n",
    "    net = models.inception_v3(pretrained=True)\n",
    "    set_parameter_requires_grad(net, feature_extract)\n",
    "    num_ftrs = net.AuxLogits.fc.in_features\n",
    "    net.AuxLogits.fc = nn.Linear(num_ftrs, out_features)\n",
    "    num_ftrs = net.fc.in_features\n",
    "    net.fc = nn.Linear(num_ftrs, out_features)\n",
    "    params_to_update = get_params_to_update(net, feature_extract)\n",
    "    optimizer = optim.Adam(params_to_update, lr=0.00002)\n",
    "    return net, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e73ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import torch.utils.data as td\n",
    "import net_training\n",
    "from birds_dataset import Birds270Dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "import pandas as pd\n",
    "\n",
    "# Splits a dataset randomly into a train and test set. The size of test set is 80% of the whole dataset.\n",
    "# Then it trains the network\n",
    "def train_net_random_dataset_split(net, dataset, epochs, optimizer, batch_size, early_stopping, is_inception):\n",
    "    train_set_size = int(len(dataset)*0.8)\n",
    "    test_set_size = len(dataset)-train_set_size\n",
    "    train_dataset, test_dataset = td.random_split(dataset, [train_set_size, test_set_size])\n",
    "    label_set = dataset.get_label_set()\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, drop_last=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "    device = None # Wybiera CUDA jeśli jest dostępne, w przeciwnym wypadku CPU\n",
    "    # device = \"cpu\" # Odkomentować jeżeli CUDA nie będzie działać\n",
    "    results = net_training.train_and_evaluate(net, train_dataloader, test_dataloader, label_set,early_stopping = early_stopping,\n",
    "                                 epochs=epochs, optimizer=optimizer, print_results=True, device=device, is_inception=is_inception)\n",
    "    return results\n",
    "\n",
    "# Creates a few networks and trains them using a random split of the dataset.\n",
    "# The number of created networks is in the \"repeat\" argument\n",
    "# It returns the final validation results for each network\n",
    "def cross_validate_net(net_generator, dataset, repeat=5, epochs=20, batch_size=32, is_inception=False):\n",
    "    all_results = []\n",
    "    for i in range(repeat):\n",
    "        print(f\"Training network {i+1} ...\")\n",
    "        net, optimizer = net_generator()\n",
    "        early_stopping = net_training.EarlyStoppingByAccuracy(patience=10)\n",
    "        results = train_net_random_dataset_split(net, dataset, epochs,optimizer=optimizer,\n",
    "                                                 early_stopping=early_stopping, batch_size=batch_size, is_inception=is_inception)\n",
    "        all_results.append(results)\n",
    "        net_training.print_final_results(results)\n",
    "        del optimizer, net\n",
    "        torch.cuda.empty_cache()\n",
    "    return all_results\n",
    "    \n",
    "    \n",
    "def print_validation_results(cross_validation_results):\n",
    "    losses = [r[\"loss\"] for r in cross_validation_results]\n",
    "    accuracies = [r[\"accuracy\"] for r in cross_validation_results]\n",
    "    print(\"Losses: \", losses)\n",
    "    print(\"Loss:  mean: {:.4f}, std: {:.4f}\".format(statistics.mean(losses), statistics.stdev(losses)))\n",
    "    print(\"Accuracies: \", accuracies)\n",
    "    print(\"Accuracy:  mean: {:.4f}, std: {:.4f}\".format(statistics.mean(accuracies), statistics.stdev(accuracies)))\n",
    "    \n",
    "def results_to_dataframe(cross_validation_results):\n",
    "    normalized = pd.json_normalize(cross_validation_results)\n",
    "    return normalized\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44012fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../data/birds270\"\n",
    "selected_birds = None\n",
    "\n",
    "#transform = transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5)) # normalizes colors to range [-1,1]\n",
    "transform =  transforms.Compose([\n",
    "    transforms.Normalize((0, 0, 0), (255, 255, 255)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_inception =  transforms.Compose([\n",
    "    transforms.Resize([299, 299]),\n",
    "    transforms.Normalize((0, 0, 0), (255, 255, 255)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# dataset = Birds270Dataset(dataset_dir,  selected_birds=selected_birds, transform=transform)\n",
    "# inception\n",
    "dataset = Birds270Dataset(dataset_dir,  selected_birds=selected_birds, transform=transform_inception)\n",
    "species_num = len(dataset.get_label_set().get_str_labels())\n",
    "def net_generator():\n",
    "    return make_inception_v3(out_features=species_num, feature_extract = False)\n",
    "\n",
    "results = cross_validate_net(net_generator, dataset, repeat=1, epochs=100, batch_size=64, is_inception=True)\n",
    "print_validation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a338172",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = results_to_dataframe(results)\n",
    "dataframe.to_csv(\"../results/inception_v3_pretrained_full_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547d404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
