{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b38a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import net_size_utils as nsu\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, kernel_size=5, channel_sizes=None):\n",
    "        super().__init__()\n",
    "        if channel_sizes == None:\n",
    "            channel_sizes = [6, 16]\n",
    "        self.channel_sizes = channel_sizes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv1 = nn.Conv2d(3, channel_sizes[0], kernel_size)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(channel_sizes[0], channel_sizes[1], kernel_size)\n",
    "        conv_height, conv_width = self.conv_out_size(224, 224)\n",
    "        linear_input_size = channel_sizes[1] * conv_width * conv_height\n",
    "        self.fc1 = nn.Linear(linear_input_size, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def conv_out_size(self, height, width):\n",
    "        dim = (height, width)\n",
    "        dim = nsu.dim_conv2d(dim, self.kernel_size)\n",
    "        dim = nsu.dim_maxpool2d(dim, 2, 2)\n",
    "        dim = nsu.dim_conv2d(dim, self.kernel_size)\n",
    "        dim = nsu.dim_maxpool2d(dim, 2, 2)\n",
    "        return dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626897ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net3Conv(nn.Module):\n",
    "    def __init__(self, kernel_size=5, channel_sizes=None):\n",
    "        super().__init__()\n",
    "        if channel_sizes == None:\n",
    "            channel_sizes = [8, 16, 32]\n",
    "        self.channel_sizes = channel_sizes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv1 = nn.Conv2d(3, channel_sizes[0], kernel_size)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(channel_sizes[0], channel_sizes[1], kernel_size)\n",
    "        self.conv3 = nn.Conv2d(channel_sizes[1], channel_sizes[2], kernel_size)\n",
    "        conv_height, conv_width = self.conv_out_size(224, 224)\n",
    "        linear_input_size = channel_sizes[2] * conv_width * conv_height\n",
    "        self.fc1 = nn.Linear(linear_input_size, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def conv_out_size(self, height, width):\n",
    "        dim = (height, width)\n",
    "        dim = nsu.dim_conv2d(dim, self.kernel_size)\n",
    "        dim = nsu.dim_maxpool2d(dim, 2, 2)\n",
    "        dim = nsu.dim_conv2d(dim, self.kernel_size)\n",
    "        dim = nsu.dim_maxpool2d(dim, 2, 2)\n",
    "        dim = nsu.dim_conv2d(dim, self.kernel_size)\n",
    "        dim = nsu.dim_maxpool2d(dim, 2, 2)\n",
    "        return dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e73ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import torch.utils.data as td\n",
    "import net_training\n",
    "from birds_dataset import Birds270Dataset\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "# Splits a dataset randomly into a train and test set. The size of test set is 80% of the whole dataset.\n",
    "# Then it trains the network\n",
    "def train_net_random_dataset_split(net, dataset, epochs, optimizer, batch_size):\n",
    "    train_set_size = int(len(dataset)*0.8)\n",
    "    test_set_size = len(dataset)-train_set_size\n",
    "    train_dataset, test_dataset = td.random_split(dataset, [train_set_size, test_set_size])\n",
    "    label_set = dataset.get_label_set()\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "    device = None # Wybiera CUDA jeśli jest dostępne, w przeciwnym wypadku CPU\n",
    "    # device = \"cpu\" # Odkomentować jeżeli CUDA nie będzie działać\n",
    "    results = net_training.train_and_evaluate(net, train_dataloader, test_dataloader, label_set,\n",
    "                                 epochs=epochs, optimizer=optimizer, device=device, print_results=True)\n",
    "    return results\n",
    "\n",
    "# Creates a few networks and trains them using a random split of the dataset.\n",
    "# The number of created networks is in the \"repeat\" argument\n",
    "# It returns the final validation results for each network\n",
    "def cross_validate_net(net_generator, dataset, repeat=5, epochs=20, batch_size=32):\n",
    "    all_results = []\n",
    "    for i in range(repeat):\n",
    "        print(f\"Training network {i+1} ...\")\n",
    "        net = net_generator()\n",
    "        results = train_net_random_dataset_split(net, dataset, epochs, optimizer=None, batch_size=batch_size)\n",
    "        all_results.append(results)\n",
    "        net_training.print_final_results(results)\n",
    "        del net\n",
    "        torch.cuda.empty_cache()\n",
    "    return all_results\n",
    "    \n",
    "    \n",
    "def print_validation_results(cross_validation_results):\n",
    "    losses = [r[\"loss\"] for r in cross_validation_results]\n",
    "    accuracies = [r[\"accuracy\"] for r in cross_validation_results]\n",
    "    print(\"Losses: \", losses)\n",
    "    print(\"Loss:  mean: {:.4f}, std: {:.4f}\".format(statistics.mean(losses), statistics.stdev(losses)))\n",
    "    print(\"Accuracies: \", accuracies)\n",
    "    print(\"Accuracy:  mean: {:.4f}, std: {:.4f}\".format(statistics.mean(accuracies), statistics.stdev(accuracies)))\n",
    "    \n",
    "def results_to_dataframe(cross_validation_results):\n",
    "    normalized = pd.json_normalize(cross_validation_results)\n",
    "    return normalized\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44012fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network 1 ...\n",
      "Epoch 0:\n",
      "\ttrain loss: 2.127503083050762\n",
      "\tvalidation loss: 1.8147951887078482, validation accuracy: 38.6986301369863%\n",
      "\tElapsed time: 0:00:12.739831\n",
      "Epoch 1:\n",
      "\ttrain loss: 1.3873731662642281\n",
      "\tvalidation loss: 1.2808435943028698, validation accuracy: 57.1917808219178%\n",
      "\tElapsed time: 0:00:12.952635\n",
      "Epoch 2:\n",
      "\ttrain loss: 0.9480015882905924\n",
      "\tvalidation loss: 1.1788051765258998, validation accuracy: 59.93150684931506%\n",
      "\tElapsed time: 0:00:13.042410\n",
      "Epoch 3:\n",
      "\ttrain loss: 0.7428801191609679\n",
      "\tvalidation loss: 1.2793030297919497, validation accuracy: 61.3013698630137%\n",
      "\tElapsed time: 0:00:12.685100\n",
      "Epoch 4:\n",
      "\ttrain loss: 0.6060818283472013\n",
      "\tvalidation loss: 1.0206948763703647, validation accuracy: 68.4931506849315%\n",
      "\tElapsed time: 0:00:12.821887\n",
      "Epoch 5:\n",
      "\ttrain loss: 0.4038067617506351\n",
      "\tvalidation loss: 1.1403333601886279, validation accuracy: 67.12328767123287%\n",
      "\tElapsed time: 0:00:12.898829\n",
      "Epoch 6:\n",
      "\ttrain loss: 0.35334507852639385\n",
      "\tvalidation loss: 1.1707068012185293, validation accuracy: 71.91780821917808%\n",
      "\tElapsed time: 0:00:12.813854\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c617cc66879b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mNet3Conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannel_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validate_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint_validation_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ad470ae81237>\u001b[0m in \u001b[0;36mcross_validate_net\u001b[1;34m(net_generator, dataset, repeat, epochs, batch_size)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Training network {i+1} ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_net_random_dataset_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mall_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mnet_training\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_final_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ad470ae81237>\u001b[0m in \u001b[0;36mtrain_net_random_dataset_split\u001b[1;34m(net, dataset, epochs, optimizer, batch_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;31m# Wybiera CUDA jeśli jest dostępne, w przeciwnym wypadku CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# device = \"cpu\" # Odkomentować jeżeli CUDA nie będzie działać\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     results = net_training.train_and_evaluate(net, train_dataloader, test_dataloader, label_set,\n\u001b[0m\u001b[0;32m     24\u001b[0m                                  epochs=epochs, optimizer=optimizer, device=device, print_results=True)\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Studia\\BIAI\\birds\\experiment\\net_training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(net, train_dataloader, test_dataloader, label_set, epochs, optimizer, early_stopping, print_results, is_inception, device)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mtrain_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_inception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[0mtrain_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mtime_elapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Studia\\BIAI\\birds\\experiment\\net_training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, train_dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dominik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dominik\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_dir = \"../data/birds270\"\n",
    "selected_birds = [\"ALBATROSS\", \"BALD EAGLE\", \"BARN OWL\", \"EURASIAN MAGPIE\", \"FLAMINGO\",\n",
    "                  \"MALLARD DUCK\", \"OSTRICH\", \"PEACOCK\", \"PELICAN\", \"TRUMPTER SWAN\"]\n",
    "transform = transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))\n",
    "\n",
    "dataset = Birds270Dataset(dataset_dir,  selected_birds=selected_birds, transform=transform)\n",
    "\n",
    "def net_generator():\n",
    "    return Net3Conv(channel_sizes=[16,16,16])\n",
    "\n",
    "results = cross_validate_net(net_generator, dataset, repeat=5, epochs=100, batch_size=64)\n",
    "print_validation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a338172",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = results_to_dataframe(results)\n",
    "dataframe.to_csv(\"../results/net_3conv_2fc_channels_16_16_16.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547d404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
